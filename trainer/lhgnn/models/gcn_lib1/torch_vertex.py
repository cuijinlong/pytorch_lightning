# 2022.06.17-Changed for building ViG model
#            Huawei Technologies Co., Ltd. <foss@huawei.com>
import numpy as np
import torch
from torch import nn
from .torch_nn import BasicConv, batched_index_select, act_layer,FFN
from .torch_edge import DenseDilatedKnnGraph, HyperedgeConstruction
from .pos_embed import get_2d_relative_pos_embed
import torch.nn.functional as F
from timm.models.layers import DropPath
import math

class LHGConv2d(nn.Module):
    """
    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type
    """
    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):
        super(LHGConv2d, self).__init__()
        self.nn = BasicConv([in_channels*3, out_channels], act, norm, bias)
        # æ¥è·å–èšç±»ä¸­å¿ƒå’Œéš¶å±åº¦æƒé‡ã€‚
        self.get_centroids = HyperedgeConstruction(in_channels)
        #self.nn_hyper = BasicConv([in_channels, in_channels], act, norm, bias)

    """
        èŠ‚ç‚¹7ï¼šè¶…å›¾å·ç§¯å®ç°
        ä½ç½®: torch_vertex.py â†’ LHGConv2d ç±»
        7.1 ä¸‰ç‰¹å¾èåˆæœºåˆ¶
        è¾“å…¥: [Batch, C, N, 1] èŠ‚ç‚¹ç‰¹å¾
        è¾“å‡º: [Batch, C, N, 1] å¢å¼ºåçš„èŠ‚ç‚¹ç‰¹å¾
        å…³é”®æŠ€æœ¯ç‚¹:
            æ¨¡ç³Šèšç±»: ä½¿ç”¨FCMç®—æ³•ç”Ÿæˆè¶…è¾¹
            ä¸‰ç‰¹å¾èåˆ: [è‡ªèº«ç‰¹å¾, é‚»å±…ç‰¹å¾, èšç±»ç‰¹å¾]
            è½¯åˆ†é…: æ¯ä¸ªèŠ‚ç‚¹ä»¥ä¸åŒç¨‹åº¦å±äºå¤šä¸ªèšç±»
    """
    def forward(self, x, edge_index, y=None,num_clusters=50,top_clusters=5, **kwargs):
        # æ­¥éª¤7.1: ä¼ ç»Ÿé‚»å±…ç‰¹å¾æå–
        x_i = batched_index_select(x, edge_index[1])
        if y is not None:
            x_j = batched_index_select(y, edge_index[0])
            
        else:
            x_j = batched_index_select(x, edge_index[0])
        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)  # æœ€å¤§ç›¸å¯¹ç‰¹å¾

        # æ­¥éª¤7.2: ğŸ¯ æ¨¡ç³Šèšç±»ç”Ÿæˆè¶…è¾¹ï¼Œè°ƒç”¨Cå‡å€¼(FCM)èšç±»ç®—æ³•è·å–è¶…è¾¹
        centroids,weights = self.get_centroids(x,num_clusters)

        # æ­¥éª¤7.3: ğŸ¯é€‰æ‹©top-kèšç±»ä¸­å¿ƒï¼Œæ ¹æ®éš¶å±åº¦æƒé‡é€‰æ‹©top-kèšç±»ä¸­å¿ƒ
        weights = weights.squeeze(-2)
        _, nn_idx_centroid = torch.topk(weights, k=top_clusters, largest=True, dim=-1)

        # æ­¥éª¤7.4: æ„å»ºè¶…å›¾è¿æ¥
        b, c, n, _ = x.shape
        center_idx = torch.arange(0, n, device=x.device).repeat(b,top_clusters, 1).transpose(2, 1)
        edge_idx = torch.stack((nn_idx_centroid, center_idx), dim=0)

        # æ­¥éª¤7.5: èšç±»ä¸­å¿ƒç‰¹å¾æå–
        x_j_cluster = batched_index_select(centroids.unsqueeze(-1), edge_idx[0])
        x_i_cluster = batched_index_select(x, edge_idx[1])
        x_j_cluster,_ = torch.max(x_j_cluster - x_i_cluster, -1, keepdim=True)

        # æ­¥éª¤7.6: ğŸ¯ ä¸‰ç‰¹å¾æ‹¼æ¥èåˆ
        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2),x_j_cluster.unsqueeze(2)], dim=2).reshape(b, 3 * c, n, -1)
        
        return self.nn(x) # æœ€ç»ˆå·ç§¯å˜æ¢
        
"""
    Max-Relativeå›¾å·ç§¯ (MRConv2d)
    è®¡ç®—èŠ‚ç‚¹ä¸é‚»å±…çš„æœ€å¤§å·®å€¼ç‰¹å¾
    å…¬å¼: max(x_j - x_i) å¢å¼ºå±€éƒ¨å·®å¼‚æ„ŸçŸ¥ 
"""
class MRConv2d(nn.Module):
    """
    Max-Relative Graph Convolution (Paper: https://arxiv.org/abs/1904.03751) for dense data type
    """
    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):
        super(MRConv2d, self).__init__()
        self.nn = BasicConv([in_channels*3, out_channels], act, norm, bias)
        self.get_centroids = HyperedgeConstruction(in_channels,'soft-kmeans')
        #self.nn_hyper = BasicConv([in_channels, in_channels], act, norm, bias)
    def forward(self, x, edge_index, y=None,num_centroids=50,H=None,W=None):
        # ğŸ¯ ä½¿ç”¨ç©ºé—´æ± åŒ–åˆå§‹åŒ–èšç±»ä¸­å¿ƒï¼ˆæ”¹è¿›çš„æ¨¡ç³Šèšç±»ï¼‰
        x_copy = x.reshape(x.shape[0],x.shape[1],H,W)
        intial_centroids = F.adaptive_avg_pool2d(x_copy, (5,10)).reshape(x.shape[0],x.shape[1],-1)

        x_i = batched_index_select(x, edge_index[1])
        if y is not None:
            x_j = batched_index_select(y, edge_index[0])
            
        else:
            x_j = batched_index_select(x, edge_index[0])
        x_j, _ = torch.max(x_j - x_i, -1, keepdim=True)

        # ğŸ¯ ä½¿ç”¨é¢„å®šä¹‰åˆå§‹ä¸­å¿ƒçš„æ¨¡ç³Šèšç±»
        if y is not None:
            centroid,weights = self.get_centroids(x,num_centroids,intial_centroids)
        else:
            centroid,weights = self.get_centroids(x,num_centroids,intial_centroids)

        #n bcentroid = self.nn_hyper(centroid.unsqueeze(-1)).squeeze(-1)
        weights = weights.squeeze(-2)
        _, nn_idx_centroid = torch.topk(weights, k=12, largest=True, dim=-1)
        b, c, n, _ = x.shape
        center_idx = torch.arange(0, n, device=x.device).repeat(b,12, 1).transpose(2, 1)
        edge_idx = torch.stack((nn_idx_centroid, center_idx), dim=0)
        x_j_center = batched_index_select(centroid.unsqueeze(-1), edge_idx[0])
        x_i_center = batched_index_select(x, edge_idx[1])
        x_j_center,_ = torch.max(x_j_center - x_i_center, -1, keepdim=True)

        x = torch.cat([x.unsqueeze(2), x_j.unsqueeze(2),x_j_center.unsqueeze(2)], dim=2).reshape(b, 3 * c, n, -1)
        #
        #x = torch.cat([x.unsqueeze(2),x_j.unsqueeze(2)], dim=2).reshape(b, 2 * c, n, -1)
        
        #max_value, _ = torch.max(self.nn(torch.cat([x_i_center, x_j_center - x_i_center], dim=1)), -1, keepdim=True)
        return self.nn(x)
        #return max_value


class EdgeConv2d(nn.Module):
    """
    Edge convolution layer (with activation, batch normalization) for dense data type
    """
    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):
        super(EdgeConv2d, self).__init__()
        self.nn = BasicConv([in_channels * 2, out_channels], act, norm, bias)

    def forward(self, x, edge_index, y=None):
        x_i = batched_index_select(x, edge_index[1])
        if y is not None:
            x_j = batched_index_select(y, edge_index[0])
        else:
            x_j = batched_index_select(x, edge_index[0])
        max_value, _ = torch.max(self.nn(torch.cat([x_i, x_j - x_i], dim=1)), -1, keepdim=True)
        return max_value


class GraphSAGE(nn.Module):
    """
    GraphSAGE Graph Convolution (Paper: https://arxiv.org/abs/1706.02216) for dense data type
    """
    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):
        super(GraphSAGE, self).__init__()
        self.nn1 = BasicConv([in_channels, in_channels], act, norm, bias)
        self.nn2 = BasicConv([in_channels*2, out_channels], act, norm, bias)

    def forward(self, x, edge_index, y=None):
        if y is not None:
            x_j = batched_index_select(y, edge_index[0])
        else:
            x_j = batched_index_select(x, edge_index[0])
        x_j, _ = torch.max(self.nn1(x_j), -1, keepdim=True)
        return self.nn2(torch.cat([x, x_j], dim=1))


class GINConv2d(nn.Module):
    """
    GIN Graph Convolution (Paper: https://arxiv.org/abs/1810.00826) for dense data type
    """
    def __init__(self, in_channels, out_channels, act='relu', norm=None, bias=True):
        super(GINConv2d, self).__init__()
        self.nn = BasicConv([in_channels, out_channels], act, norm, bias)
        eps_init = 0.0
        self.eps = nn.Parameter(torch.Tensor([eps_init]))

    def forward(self, x, edge_index, y=None):
        if y is not None:
            x_j = batched_index_select(y, edge_index[0])
        else:
            x_j = batched_index_select(x, edge_index[0])
        x_j = torch.sum(x_j, -1, keepdim=True)
        return self.nn((1 + self.eps) * x + x_j)


class GraphConv2d(nn.Module):
    """
    Static graph convolution layer
    """
    def __init__(self, in_channels, out_channels, conv='edge', act='relu', norm=None, bias=True):
        super(GraphConv2d, self).__init__()
        if conv == 'edge':
            self.gconv = EdgeConv2d(in_channels, out_channels, act, norm, bias)
        elif conv == 'mr':
            """
                Max-Relativeå›¾å·ç§¯ (MRConv2d)
                è®¡ç®—èŠ‚ç‚¹ä¸é‚»å±…çš„æœ€å¤§å·®å€¼ç‰¹å¾
                å…¬å¼: max(x_j - x_i) å¢å¼ºå±€éƒ¨å·®å¼‚æ„ŸçŸ¥ 
            """
            self.gconv = MRConv2d(in_channels, out_channels, act, norm, bias)
        elif conv == 'sage':
            self.gconv = GraphSAGE(in_channels, out_channels, act, norm, bias)
        elif conv == 'gin':
            self.gconv = GINConv2d(in_channels, out_channels, act, norm, bias)
        elif conv == 'lhg':
            """
            è¶…å›¾å·ç§¯ (LHGConv2d) - æ ¸å¿ƒåˆ›æ–°
            èåˆä¸‰ç§ç‰¹å¾:
                1. èŠ‚ç‚¹è‡ªèº«ç‰¹å¾
                2. é‚»å±…å·®å€¼ç‰¹å¾  
                3. èšç±»ä¸­å¿ƒå·®å€¼ç‰¹å¾
            è¶…å›¾æ„å»ºæµç¨‹ï¼š
                soft-kmeansèšç±»ç”Ÿæˆè¶…è¾¹ï¼ˆèšç±»ä¸­å¿ƒï¼‰
                top-ké€‰æ‹©ä»èšç±»ä¸­å¿ƒé€‰å–ä»£è¡¨æ€§èŠ‚ç‚¹
                ä¸‰ç‰¹å¾æ‹¼æ¥ï¼š[è‡ªèº«, é‚»å±…å·®, ä¸­å¿ƒå·®]
            """
            self.gconv = LHGConv2d(in_channels, out_channels, act, norm, bias)
        else:
            raise NotImplementedError('conv:{} is not supported'.format(conv))

    def forward(self, x, edge_index, y=None,**kwargs):
        return self.gconv(x, edge_index, y,**kwargs)

class DyGraphConv2d(GraphConv2d):
    """
        åŠ¨æ€å›¾å·ç§¯è°ƒç”¨é“¾
    """
    def __init__(self,
                 in_channels,
                 out_channels,
                 kernel_size=9,
                 dilation=1,
                 conv='edge',
                 act='relu',
                 norm=None,
                 bias=True,
                 stochastic=False,
                 epsilon=0.0,
                 r=1):
        super(DyGraphConv2d, self).__init__(in_channels, out_channels, conv, act, norm, bias)
        self.k = kernel_size
        self.d = dilation
        self.r = r
        # DyGraphConv2dä¸­åŠ¨æ€è®¡ç®—KNNå›¾
        """
           è°ƒç”¨dilated_knn_graphæ„å»ºå›¾ï¼ˆedge_indexï¼‰
               dilationè†¨èƒ€ç‡æ§åˆ¶:ï¼ˆidx:backboneçš„Seqå±‚æ•° idx=0-3:  è†¨èƒ€ç‡=1ï¼Œ idx=4-7:  è†¨èƒ€ç‡=2ï¼Œidx=8-11: è†¨èƒ€ç‡=3...ä»¥æ­¤ç±»æ¨ï¼Œ
               stochasticï¼šTrue:éšæœºé‡‡æ · False:è§„åˆ™è†¨èƒ€é‡‡æ ·
               
               å®é™…é‚»å±…é€‰æ‹©è¿‡ç¨‹:
                    æ¨¡å—é˜¶æ®µ	kå€¼	    dilationå€¼	    å€™é€‰é‚»å±…æ•°	æœ€ç»ˆé‚»å±…æ•°
                    ç¬¬1é˜¶æ®µ	10	    1	            10Ã—1=10	    10
                    ç¬¬2é˜¶æ®µ	10	    2	            10Ã—2=20	    10
                    ç¬¬3é˜¶æ®µ	10	    3	            10Ã—3=30	    10
                å¯¹äº dilation=2 çš„æ¨¡å—:
                    å€™é€‰é‚»å±…ï¼ˆæŒ‰è·ç¦»æ’åºï¼‰: [A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T]
                    ç´¢å¼•ä½ç½®:              0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,11,12,13,14,15,16,17,18,19
                    è†¨èƒ€é‡‡æ ·ï¼ˆæ¯éš”2ä¸ªé€‰1ä¸ªï¼‰: 0, 2, 4, 6, 8, 10, 12, 14, 16, 18
                    å¯¹åº”é‚»å±…:               A, C, E, G, I, K,  M,  O,  Q,  S
        """
        self.dilated_knn_graph = DenseDilatedKnnGraph(kernel_size, dilation, stochastic, epsilon)

    """
    èŠ‚ç‚¹6ï¼šåŠ¨æ€å›¾å·ç§¯æ ¸å¿ƒ
    ä½ç½®: torch_vertex.py â†’ DyGraphConv2d ç±»
        6.1 å›¾æ„å»ºè¿‡ç¨‹
        è¾“å…¥: [Batch, C, H, W] ç©ºé—´ç‰¹å¾å›¾
        è¾“å‡º: [Batch, C*channel_mul, H, W] å›¾å·ç§¯è¾“å‡º
        å…³é”®æŠ€æœ¯ç‚¹:
            èŠ‚ç‚¹åŒ–å¤„ç†: å°†ç©ºé—´ä½ç½®è§†ä¸ºå›¾èŠ‚ç‚¹
            è†¨èƒ€KNN: æ‰©å¤§æ„Ÿå—é‡è€Œä¸å¢åŠ å‚æ•°
            å¤šå°ºåº¦å›¾: é€šè¿‡ä¸‹é‡‡æ ·æ„å»ºå±‚æ¬¡åŒ–å›¾ç»“æ„
    """
    def forward(self, x, relative_pos=None, **kwargs):
        # kwargs:{ num_clusters: [32(LHGNN.yamlé…ç½®clustersæ•°),..... sum(self.blocks)å—æ•°], top_clusters: math.ceil(num_knn(LHGNN.yaml.k) * cluster_ratio(LHGNN.yaml.cluster_ratio)) }
        B, C, H, W = x.shape
        # æ­¥éª¤6.1: ä¸‹é‡‡æ ·æ„å»ºå¤šå°ºåº¦å›¾
        y = None
        if self.r > 1:
            y = F.avg_pool2d(x, self.r, self.r)  # ç©ºé—´ä¸‹é‡‡æ ·
            y = y.reshape(B, C, -1, 1).contiguous()

        # æ­¥éª¤6.2: å±•å¹³ç‰¹å¾ä¸ºèŠ‚ç‚¹æ ¼å¼
        x = x.reshape(B, C, -1, 1).contiguous()
        """
            æ­¥éª¤6.3: KNNå›¾æ„å»º
            å‡è®¾ä½ åœ¨ä¸€ä¸ªå¤§å‹ç›¸äº²äº¤å‹æ´»åŠ¨ä¸­, ä½ éœ€è¦å¸®æ¯ä¸ªäººæ‰¾åˆ° kï¼ˆLHGNN.yamlï¼‰ ä¸ªæœ€åˆé€‚çš„æœ‹å‹/ä¼´ä¾£ï¼Œæ ‡å‡†æ˜¯ï¼š
                é•¿ç›¸è¦ç›¸ä¼¼ï¼ˆç‰¹å¾ç›¸ä¼¼ï¼‰
                ä½å¾—è¦è¿‘ï¼ˆç©ºé—´ä½ç½®æ¥è¿‘ï¼‰
        """
        edge_index = self.dilated_knn_graph(x = x, y = y, relative_pos = relative_pos) # xï¼š[8, 80, 1024, 1] y: [8, 80, 64, 1] relative_pos: [1, 1024, 64]
        # æ­¥éª¤6.4: æ‰§è¡Œå›¾å·ç§¯
        """
            æ¯ä¸ªåƒç´ ç‚¹å«ä¸Šè‡ªå·±çš„kä¸ªå¥½æœ‹å‹ï¼Œå¤§å®¶ååœ¨ä¸€èµ·å¼€å­¦ä¹ äº¤æµä¼šï¼Œæ¯”è¾ƒå½¼æ­¤çš„ä¼˜ç¼ºç‚¹ï¼Œå¸æ”¶åˆ«äººçš„é•¿å¤„ï¼Œæ›´æ–°è‡ªå·±çš„çŸ¥è¯†å‚¨å¤‡ï¼Œç„¶åå›åˆ°åŸæ¥çš„ä½ç½®ç»§ç»­å·¥ä½œ
            ä¸‰ç‰¹å¾èåˆï¼š
                1. è‡ªèº«ç‰¹å¾ï¼šåŸå§‹ç‰¹å¾
                2. é‚»å±…ç‰¹å¾ï¼šå±€éƒ¨å¯¹æ¯”å·®å¼‚  
                3. èšç±»ç‰¹å¾ï¼šå…¨å±€æ¦œæ ·æŒ‡å¯¼
            æ¨¡ç³Šèšç±»ï¼šè½¯åˆ†é…ï¼Œæ¯ä¸ªèŠ‚ç‚¹å±äºå¤šä¸ªèšç±»(Cå‡å€¼æ¨¡ç³Šèšç±»)
            é€šé“æ‰©å±•ï¼š80â†’240â†’80ï¼ˆå‘æ•£-æ”¶æ•›æ€ç»´ï¼‰
        """
        x = super(DyGraphConv2d, self).forward(x, edge_index, y, **kwargs) # GraphConv2d ->  LHGConv2d -> ä¸‰ä¸ªç‰¹å¾èåˆ
        # è¾“å…¥: [8, 80, 1024, 1]  # 80ä¸ªç‰¹å¾ï¼Œ1024ä¸ªåƒç´  ->  æ‹¼æ¥ä¸‰ä»½ç‰¹å¾çŸ©é˜µï¼š è¾“å‡º: [8, 240, 1024, 1]  # 80Ã—3=240ä¸ªç‰¹å¾ -> reshapeæ¢å¤ç©ºé—´å½¢çŠ¶ï¼š æœ€ç»ˆ: [8, 240, 16, 64]   # 240ä¸ªç‰¹å¾é€šé“
        # æ­¥éª¤6.5: æ¢å¤ç©ºé—´æ ¼å¼
        # äº¤æµä¼šç»“æŸï¼ŒåŒå­¦ä»¬å¸¦ç€æ–°çŸ¥è¯†å›åˆ°è‡ªå·±åº§ä½ [8, 240, 16, 64]
        z = x.reshape(B, -1, H, W).contiguous()
        return z

"""
ç»“æ„ä¸º fc1 -> graph_conv -> fc2 -> drop_pathï¼š
fc1 ä¸ fc2ï¼šå‡ä¸º 1Ã—1 å·ç§¯ + BatchNormã€‚
    fc1ï¼šä¿æŒé€šé“æ•°ä¸å˜ï¼Œå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œçº¿æ€§å˜æ¢ï¼Œä¸ºå›¾å·ç§¯åšå‡†å¤‡ã€‚
    fc2ï¼šå°†å›¾å·ç§¯è¾“å‡ºæ˜ å°„å›åŸé€šé“æ•°ï¼Œç¡®ä¿æ®‹å·®è¿æ¥ç»´åº¦åŒ¹é…ã€‚
    
graph_convï¼ˆåŠ¨æ€å›¾å·ç§¯ï¼‰ï¼šæ ¹æ®é…ç½®ä½¿ç”¨ä¸åŒçš„å›¾å·ç§¯ç±»å‹ï¼ˆç”± conv å‚æ•°æ§åˆ¶ï¼‰ï¼Œæ ¸å¿ƒæ˜¯å»ºæ¨¡èŠ‚ç‚¹ä¸å…¶é‚»å±…çš„å…³ç³»ï¼š
    Max-Relative å›¾å·ç§¯ï¼ˆMRConv2dï¼‰ï¼šè®¡ç®—èŠ‚ç‚¹ x_i ä¸å…¶é‚»å±… x_j çš„å·®å€¼ x_j - x_iï¼Œå–æœ€å¤§å€¼ä½œä¸ºé‚»å±…ç‰¹å¾ï¼Œå†ä¸èŠ‚ç‚¹è‡ªèº«ç‰¹å¾æ‹¼æ¥ï¼ˆ[x, x_j - x_i]ï¼‰ï¼Œå¢å¼ºå±€éƒ¨å·®å¼‚æ„ŸçŸ¥ã€‚
    è¶…å›¾å·ç§¯ï¼ˆLHGConvï¼‰ï¼šå½“ conv='lhg' æ—¶å¯ç”¨ï¼Œé€šè¿‡ soft-kmeans èšç±»ç”Ÿæˆ â€œè¶…è¾¹â€ï¼ˆèšç±»ä¸­å¿ƒï¼‰ï¼ŒèåˆèŠ‚ç‚¹è‡ªèº«ç‰¹å¾ã€é‚»å±…å·®å€¼ç‰¹å¾ã€ä¸­å¿ƒå·®å€¼ç‰¹å¾ï¼ˆå…± 3 ç±»ç‰¹å¾ï¼‰ï¼Œæå‡å…¨å±€å…³ç³»å»ºæ¨¡èƒ½åŠ›ã€‚

DropPathï¼šéšæœºæ·±åº¦æœºåˆ¶ï¼Œä»¥ä¸€å®šæ¦‚ç‡ä¸¢å¼ƒå½“å‰æ¨¡å—è¾“å‡ºï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼ˆæ·±å±‚æ¨¡å—ä¸¢å¼ƒæ¦‚ç‡æ›´é«˜ï¼‰ã€‚
æ®‹å·®è¿æ¥ï¼šå°†è¾“å…¥ä¸å¤„ç†åçš„ç‰¹å¾ç›¸åŠ ï¼ˆx = DropPath(fc2(graph_conv(fc1(x)))) + xï¼‰ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
"""
class Grapher(nn.Module):
    """
    Grapher module with graph convolution and fc layers
    """
    def __init__(self,
                 in_channels=1,
                 num_knn=9,
                 num_clusters=50,
                 dilation=1,
                 conv='edge',
                 act='relu',
                 norm=None,
                 bias=True,
                 stochastic=False,
                 epsilon=0.0,
                 r=1,
                 n=196,
                 drop_path=0.0,
                 relative_pos=False,
                 cluster_ratio=0.5,
                 channel_mul=1):
        super(Grapher, self).__init__()
        self.channels = in_channels
        self.n = n
        self.r = r
        channel_mul= int(channel_mul)
        self.conv = conv 
        self.num_clusters = num_clusters
        self.fc1 = nn.Sequential(
            nn.Conv2d(in_channels, in_channels, 1, stride=1, padding=0),
            nn.BatchNorm2d(in_channels),
        )
        """
            å¤§å‹ç›¸äº²äº¤å‹æ´»åŠ¨æ‰§è¡Œè¿‡ç¨‹
        """
        self.graph_conv = DyGraphConv2d(in_channels, in_channels * channel_mul,
                                        num_knn, dilation, conv,
                                        act, norm, bias, stochastic,
                                        epsilon, r)
        self.fc2 = nn.Sequential(
            nn.Conv2d(in_channels * channel_mul, in_channels, 1, stride=1, padding=0),
            nn.BatchNorm2d(in_channels),
        )
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.relative_pos = None
        self.cluster_ratio = cluster_ratio
        # ä»èšç±»ä¸­å¿ƒé€‰æ‹©çš„top-kæ•°é‡
        self.top_clusters = math.ceil(num_knn * cluster_ratio)  # = ceil(10 Ã— 0.4) = 4
        if relative_pos:
            """
                ä¸º2Dç½‘æ ¼ä¸Šçš„æ¯ä¸ªä½ç½®ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ç¼–ç å‘é‡
                ç¼–ç åº”è¯¥èƒ½å¤Ÿåæ˜ ä½ç½®ä¹‹é—´çš„ç©ºé—´å…³ç³»
                ç›¸è¿‘ä½ç½®ç¼–ç ç›¸ä¼¼ï¼Œè¿œç¦»ä½ç½®ç¼–ç å·®å¼‚å¤§
            """
            # 1. ç”Ÿæˆæ­£å¼¦-ä½™å¼¦ç»å¯¹ä½ç½®ç¼–ç 
            pos_embed = get_2d_relative_pos_embed(embed_dim = in_channels, grid_size = int(n**0.5)) # [1024, 1024]
            relative_pos_tensor = torch.from_numpy(np.float32(pos_embed)).unsqueeze(0).unsqueeze(1)

            relative_pos_tensor = F.interpolate(relative_pos_tensor,
                                                size=(n, n//(r*r)),
                                                mode='bicubic',
                                                align_corners=False)
            self.relative_pos = nn.Parameter(-relative_pos_tensor.squeeze(1), requires_grad=False) # torch.Size()

    def _get_relative_pos(self, relative_pos, H, W):
        if relative_pos is None or H * W == self.n: # æƒ…å†µ1ï¼šä¸éœ€è¦ä½ç½®ç¼–ç ï¼Œæˆ–å°ºå¯¸åŒ¹é…é¢„è®¾å€¼
            return relative_pos
        else:  # æƒ…å†µ2ï¼šç‰¹å¾å›¾å°ºå¯¸å˜åŒ–ï¼Œéœ€è¦æ’å€¼è°ƒæ•´
            N = H * W # å½“å‰å®é™…èŠ‚ç‚¹æ•°
            N_reduced = N // (self.r * self.r) # ä¸‹é‡‡æ ·åçš„èŠ‚ç‚¹æ•°
            return F.interpolate(relative_pos.unsqueeze(0), size=(N, N_reduced), mode="bicubic").squeeze(0)

    """
        èŠ‚ç‚¹5ï¼šGrapherå›¾å·ç§¯æ¨¡å—
        ä½ç½®: torch_vertex.py â†’ Grapher ç±»
        è¾“å…¥: [Batch, C, H, W] å½“å‰stageçš„ç‰¹å¾å›¾
        è¾“å‡º: [Batch, C, H, W] å›¾å·ç§¯å¤„ç†åçš„ç‰¹å¾å›¾
        å…³é”®æŠ€æœ¯ç‚¹:
            åŠ¨æ€å›¾æ„å»º: æ¯å±‚æ ¹æ®ç‰¹å¾åŠ¨æ€è®¡ç®—KNNå›¾
            è¶…å›¾å·ç§¯: ä½¿ç”¨æ¨¡ç³Šèšç±»ç”Ÿæˆè¶…è¾¹
            æ®‹å·®è¿æ¥: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
    """
    def forward(self, x):
        _tmp = x  # ä¿å­˜æ®‹å·®è¿æ¥ ä¾‹ï¼štorch.Size([8, 80, 16, 64]) 8å¼ å›¾ï¼Œæ¯å¼ å›¾æœ‰80ä¸ªç‰¹å¾é€šé“ï¼Œåˆ†è¾¨ç‡16Ã—64åƒç´ ï¼Œ
        # æ­¥éª¤5.1: çº¿æ€§å˜æ¢ï¼Œfc1æ˜¯ä¸€ä¸ª1Ã—1å·ç§¯ + BatchNormï¼Œä½œç”¨æ˜¯ï¼š1.å¯¹æ¯ä¸ªä½ç½®çš„80ä¸ªç‰¹å¾è¿›è¡Œçº¿æ€§ç»„åˆï¼›2.ä¿æŒè¾“å…¥è¾“å‡ºå°ºå¯¸ä¸å˜ï¼Œåªåšç‰¹å¾å˜æ¢ï¼›3.ç±»ä¼¼äºTransformerä¸­çš„ä½ç½®å‰é¦ˆç½‘ç»œ 4.å°±åƒç»™æ¯ä¸ªåƒç´ ç‚¹çš„ç‰¹å¾åšä¸€ä¸ª"ç‰¹å¾é‡ç»„"ã€‚
        x = self.fc1(x) # torch.Size([8, 80, 16, 64]) [B,C,H,W] â†’ [B,C,H,W] (1Ã—1å·ç§¯)
        # æ­¥éª¤5.2: åŠ¨æ€å›¾å·ç§¯
        B, C, H, W = x.shape # B=8 (batch_size), C=80 (channels), H=16 (height), W=64 (width) æ€»åƒç´ ç‚¹ï¼ˆèŠ‚ç‚¹æ•°ï¼‰N = H Ã— W = 16 Ã— 64 = 1024
        """
            é—®é¢˜ï¼šå›¾å·ç§¯å°†åƒç´ çœ‹ä½œ"èŠ‚ç‚¹"ï¼Œä½†å¤±å»äº†åƒç´ é—´çš„ç©ºé—´ä½ç½®å…³ç³»ï¼›
            è§£å†³æ–¹æ¡ˆï¼šæ·»åŠ ä½ç½®ç¼–ç ï¼Œå‘Šè¯‰æ¨¡å‹"å“ªäº›åƒç´ åœ¨ç©ºé—´ä¸Šé è¿‘"ï¼›
            ç±»æ¯”ï¼šå°±åƒç»™åœ°å›¾ä¸Šçš„æ¯ä¸ªç‚¹æ ‡è®°ç»çº¬åº¦åæ ‡
        """
        relative_pos = self._get_relative_pos(self.relative_pos, H, W) # relative_posï¼štorch.Size([1, 1024, 64]) Hï¼š16 Wï¼š64 -> relative_posï¼štorch.Size([1, 1024, 64])
        if self.conv == 'lhg':
            # è¶…å›¾å·ç§¯åˆ†æ”¯
            """
               å¤§å‹ç›¸äº²äº¤å‹æ´»åŠ¨æ‰§è¡Œè¿‡ç¨‹0
            """
            x = self.graph_conv(x=x, relative_pos=relative_pos, num_clusters=self.num_clusters, top_clusters=self.top_clusters)
        else:
            x = self.graph_conv(x=x, relative_pos=relative_pos)
        # æ­¥éª¤5.3: çº¿æ€§å˜æ¢æ¢å¤é€šé“
        x = self.fc2(x) # [B,C*channel_mul,H,W] â†’ [B,C,H,W]
        # æ­¥éª¤5.4: DropPath + æ®‹å·®è¿æ¥
        x = self.drop_path(x) + _tmp
        x = x.reshape(B, C, H, W)
        return x