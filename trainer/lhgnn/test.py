# test.py
from typing import Any, Dict, Optional
import hydra
import lightning as L
import torch
import logging
from pytorch_lightning import LightningDataModule, LightningModule, Trainer, Callback
from pytorch_lightning.loggers import Logger
from omegaconf import DictConfig
import rootutils
import os

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½• - ä¸ train.py ä¿æŒä¸€è‡´
rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True, cwd=False)
os.environ['HYDRA_FULL_ERROR'] = '1'

from trainer.lhgnn.models.utils import (
    RankedLogger,
    extras,
    instantiate_callbacks,
    instantiate_loggers,
    log_hyperparameters,
    task_wrapper,
)

log = RankedLogger(__name__, rank_zero_only=True)


@task_wrapper
def test(cfg: DictConfig) -> Dict[str, Any]:
    """ä¸“é—¨ç”¨äºæ¨¡å‹æµ‹è¯•çš„å‡½æ•°

    :param cfg: Hydra é…ç½®å¯¹è±¡
    :return: æµ‹è¯•æŒ‡æ ‡å­—å…¸
    """
    # è®¾ç½®éšæœºç§å­
    if cfg.get("seed"):
        L.seed_everything(cfg.seed, workers=True)

    # å®ä¾‹åŒ–æ•°æ®æ¨¡å—
    log.info(f"Instantiating datamodule <{cfg.data._target_}>")
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)

    # å®ä¾‹åŒ–æ¨¡å‹
    log.info(f"Instantiating model <{cfg.model._target_}>")
    model: LightningModule = hydra.utils.instantiate(cfg.model)

    # å®ä¾‹åŒ–å›è°ƒå‡½æ•°
    log.info("Instantiating callbacks...")
    callbacks = instantiate_callbacks(cfg.get("callbacks"))

    # å®ä¾‹åŒ–æ—¥å¿—è®°å½•å™¨
    log.info("Instantiating loggers...")
    logger = instantiate_loggers(cfg.get("logger"))

    # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹ - ä¸“é—¨ç”¨äºæµ‹è¯•
    log.info(f"Instantiating trainer for testing...")
    trainer: Trainer = hydra.utils.instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=logger
    )

    # è®°å½•æ‰€æœ‰å®ä¾‹åŒ–å¯¹è±¡
    object_dict = {
        "cfg": cfg,
        "datamodule": datamodule,
        "model": model,
        "logger": logger,
        "callbacks": callbacks,
        "trainer": trainer,
    }

    if logger:
        log.info("Logging hyperparameters!")
        log_hyperparameters(object_dict)

    # æ¨¡å‹æµ‹è¯•é€»è¾‘
    test_results = None

    # æ–¹å¼1ï¼šä½¿ç”¨åŠ æƒå¹³å‡æ¨¡å‹è¿›è¡Œæµ‹è¯•
    if cfg.get("wa"):
        log.info("Testing with weighted average model")
        model_ckpt = []
        ckpt_dir = cfg.callbacks.model_checkpoint.dirpath

        # é‡æ–°å®ä¾‹åŒ–æ¨¡å‹ç”¨äºåŠ æƒå¹³å‡
        model_wa: LightningModule = hydra.utils.instantiate(cfg.model)
        own_state = model_wa.state_dict()

        # æ”¶é›†æ‰€æœ‰æ£€æŸ¥ç‚¹
        for ckpt_file in os.listdir(ckpt_dir):
            if 'ckpt' in ckpt_file:
                ckpt_path = os.path.join(ckpt_dir, ckpt_file)
                model_ckpt.append(torch.load(ckpt_path)['state_dict'])

        # è®¡ç®—åŠ æƒå¹³å‡
        for name, params in own_state.items():
            own_state[name] = torch.zeros_like(params)
            model_ckpt_key = torch.cat([d[name].float().unsqueeze(0) for d in model_ckpt], dim=0)
            own_state[name].copy_(torch.mean(model_ckpt_key, dim=0))

        model_wa.load_state_dict(own_state)

        # æ‰§è¡Œæµ‹è¯•
        test_results = trainer.test(model=model_wa, datamodule=datamodule)
        log.info(f"Test results from weighted average model: {test_results}")

    # æ–¹å¼2ï¼šä½¿ç”¨æŒ‡å®šçš„æ£€æŸ¥ç‚¹è¿›è¡Œæµ‹è¯•
    elif cfg.get("ckpt_path"):
        ckpt_path = cfg.ckpt_path
        log.info(f"Testing with specified checkpoint: {ckpt_path}")

        if os.path.exists(ckpt_path):
            test_results = trainer.test(
                model=model,
                datamodule=datamodule,
                ckpt_path=ckpt_path
            )
            log.info(f"Test results with specified checkpoint: {test_results}")
        else:
            log.error(f"Checkpoint path does not exist: {ckpt_path}")

    # æ–¹å¼3ï¼šä½¿ç”¨æœ€ä½³æ£€æŸ¥ç‚¹è¿›è¡Œæµ‹è¯•
    elif hasattr(trainer, 'checkpoint_callback') and trainer.checkpoint_callback:
        ckpt_path = trainer.checkpoint_callback.best_model_path
        if ckpt_path and os.path.exists(ckpt_path):
            log.info(f"Testing with best checkpoint: {ckpt_path}")
            test_results = trainer.test(
                model=model,
                datamodule=datamodule,
                ckpt_path=ckpt_path
            )
            log.info(f"Test results with best checkpoint: {test_results}")
        else:
            log.warning("No best checkpoint found, testing with current model weights")
            test_results = trainer.test(model=model, datamodule=datamodule)
            log.info(f"Test results with current weights: {test_results}")

    # æ–¹å¼4ï¼šä½¿ç”¨å½“å‰æ¨¡å‹æƒé‡è¿›è¡Œæµ‹è¯•
    else:
        log.info("Testing with current model weights")
        test_results = trainer.test(model=model, datamodule=datamodule)
        log.info(f"Test results with current weights: {test_results}")

    log.info("Testing completed!")
    return test_results[0] if test_results and len(test_results) > 0 else {}


@hydra.main(version_base="1.3", config_path="./configs", config_name="test.yaml")
def main(cfg: DictConfig) -> None:
    """æµ‹è¯•ä¸»å…¥å£ç‚¹

    :param cfg: Hydra é…ç½®å¯¹è±¡
    """
    # åº”ç”¨é¢å¤–å·¥å…·ï¼ˆä¸ train.py ä¿æŒä¸€è‡´ï¼‰
    extras(cfg)

    log.info("Starting model testing process")

    # æ‰§è¡Œæµ‹è¯•
    test_results = test(cfg)

    # æ‰“å°ä¸»è¦æµ‹è¯•æŒ‡æ ‡
    if test_results:
        log.info("=== Final Test Results ===")
        for metric_name, metric_value in test_results.items():
            log.info(f"{metric_name}: {metric_value:.4f}")

        # ç‰¹åˆ«å…³æ³¨ mAP æŒ‡æ ‡
        if 'mAP' in test_results:
            log.info(f"ğŸ¯ Test mAP: {test_results['mAP']:.4f}")
    else:
        log.warning("No test results obtained")


if __name__ == "__main__":
    torch.set_float32_matmul_precision("high")
    main()