# test.py
from typing import Any, Dict, Optional, Tuple
import hydra
import lightning as L
import torch
import logging
from pytorch_lightning import LightningDataModule, LightningModule, Trainer, Callback
from pytorch_lightning.loggers import Logger
from omegaconf import DictConfig
import rootutils
import os
import numpy as np

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True, cwd=False)
os.environ['HYDRA_FULL_ERROR'] = '1'

from trainer.lhgnn.models.utils import (
    RankedLogger,
    extras,
    instantiate_callbacks,
    instantiate_loggers,
    log_hyperparameters,
    task_wrapper,
)

log = RankedLogger(__name__, rank_zero_only=True)


def load_pretrained_weights(model: LightningModule, pretrain_path: str) -> None:
    """åŠ è½½é¢„è®­ç»ƒæƒé‡"""
    log.info(f"Loading pretrained weights from: {pretrain_path}")

    if not os.path.exists(pretrain_path):
        log.error(f"Pretrain path does not exist: {pretrain_path}")
        return

    try:
        checkpoint = torch.load(pretrain_path, map_location="cpu")

        # å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼
        if "state_dict" in checkpoint:
            state_dict = checkpoint["state_dict"]
        else:
            state_dict = checkpoint

        # ç§»é™¤å¯èƒ½çš„æ¨¡å—å‰ç¼€
        new_state_dict = {}
        for k, v in state_dict.items():
            if k.startswith("model.") or k.startswith("module."):
                new_state_dict[k.replace("model.", "").replace("module.", "")] = v
            else:
                new_state_dict[k] = v

        # åŠ è½½æƒé‡ï¼Œå…è®¸éƒ¨åˆ†åŒ¹é…
        missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)

        if missing_keys:
            log.warning(f"Missing keys in pretrained weights: {missing_keys}")
        if unexpected_keys:
            log.warning(f"Unexpected keys in pretrained weights: {unexpected_keys}")

        log.info("Pretrained weights loaded successfully")

    except Exception as e:
        log.error(f"Failed to load pretrained weights: {e}")


def create_weighted_average_model(cfg: DictConfig, model: LightningModule, ckpt_dir: str) -> LightningModule:
    """åˆ›å»ºåŠ æƒå¹³å‡æ¨¡å‹"""
    log.info("Creating weighted average model")
    model_ckpt = []

    # æ”¶é›†æ‰€æœ‰æ£€æŸ¥ç‚¹
    for ckpt_file in os.listdir(ckpt_dir):
        if ckpt_file.endswith('.ckpt') and ckpt_file != 'wa.pth.tar':
            ckpt_path = os.path.join(ckpt_dir, ckpt_file)
            try:
                checkpoint = torch.load(ckpt_path, map_location="cpu")
                if "state_dict" in checkpoint:
                    model_ckpt.append(checkpoint["state_dict"])
                    log.info(f"Loaded checkpoint: {ckpt_file}")
            except Exception as e:
                log.warning(f"Failed to load checkpoint {ckpt_file}: {e}")

    if not model_ckpt:
        log.error("No valid checkpoints found for weighted average")
        return model

    # é‡æ–°å®ä¾‹åŒ–æ¨¡å‹ç”¨äºåŠ æƒå¹³å‡
    model_wa: LightningModule = hydra.utils.instantiate(cfg.model)
    own_state = model_wa.state_dict()

    # è®¡ç®—åŠ æƒå¹³å‡
    log.info(f"Averaging {len(model_ckpt)} checkpoints")
    for name, params in own_state.items():
        if name in model_ckpt[0]:
            own_state[name] = torch.zeros_like(params)
            model_ckpt_key = torch.stack([d[name].float() for d in model_ckpt], dim=0)
            own_state[name].copy_(torch.mean(model_ckpt_key, dim=0))

    model_wa.load_state_dict(own_state)
    log.info("Weighted average model created successfully")

    return model_wa


@task_wrapper
def test(cfg: DictConfig) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """ä¸“é—¨ç”¨äºæ¨¡å‹æµ‹è¯•çš„å‡½æ•°

    :param cfg: Hydra é…ç½®å¯¹è±¡
    :return: æµ‹è¯•æŒ‡æ ‡å­—å…¸å’Œå¯¹è±¡å­—å…¸
    """
    # è®¾ç½®éšæœºç§å­
    if cfg.get("seed"):
        L.seed_everything(cfg.seed, workers=True)

    # å®ä¾‹åŒ–æ•°æ®æ¨¡å—
    log.info(f"Instantiating datamodule <{cfg.data._target_}>")
    datamodule: LightningDataModule = hydra.utils.instantiate(cfg.data)

    # å®ä¾‹åŒ–æ¨¡å‹
    log.info(f"Instantiating model <{cfg.model._target_}>")
    model: LightningModule = hydra.utils.instantiate(cfg.model)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æœæä¾›ï¼‰
    if cfg.get("pretrain_path"):
        load_pretrained_weights(model, cfg.pretrain_path)

    # å®ä¾‹åŒ–å›è°ƒå‡½æ•°
    log.info("Instantiating callbacks...")
    callbacks = instantiate_callbacks(cfg.get("callbacks"))

    # å®ä¾‹åŒ–æ—¥å¿—è®°å½•å™¨
    log.info("Instantiating loggers...")
    logger = instantiate_loggers(cfg.get("logger"))

    # åˆ›å»ºè®­ç»ƒå™¨å®ä¾‹ - ä¸“é—¨ç”¨äºæµ‹è¯•
    log.info(f"Instantiating trainer for testing...")
    trainer: Trainer = hydra.utils.instantiate(
        cfg.trainer,
        callbacks=callbacks,
        logger=logger
    )

    # è®°å½•æ‰€æœ‰å®ä¾‹åŒ–å¯¹è±¡
    object_dict = {
        "cfg": cfg,
        "datamodule": datamodule,
        "model": model,
        "logger": logger,
        "callbacks": callbacks,
        "trainer": trainer,
    }

    if logger:
        log.info("Logging hyperparameters!")
        log_hyperparameters(object_dict)

    # æ¨¡å‹æµ‹è¯•é€»è¾‘
    test_results = None

    # æ–¹å¼1ï¼šä½¿ç”¨åŠ æƒå¹³å‡æ¨¡å‹è¿›è¡Œæµ‹è¯•
    if cfg.get("wa"):
        log.info("Testing with weighted average model")

        # è·å–æ£€æŸ¥ç‚¹ç›®å½•
        ckpt_dir = cfg.callbacks.model_checkpoint.dirpath
        if not os.path.exists(ckpt_dir):
            log.error(f"Checkpoint directory does not exist: {ckpt_dir}")
            return {}, object_dict

        model_wa = create_weighted_average_model(cfg, model, ckpt_dir)
        test_results = trainer.test(model=model_wa, datamodule=datamodule)
        log.info(f"Test results from weighted average model: {test_results}")

    # æ–¹å¼2ï¼šä½¿ç”¨æŒ‡å®šçš„æ£€æŸ¥ç‚¹è¿›è¡Œæµ‹è¯•
    elif cfg.get("ckpt_path"):
        ckpt_path = cfg.ckpt_path
        log.info(f"Testing with specified checkpoint: {ckpt_path}")

        if os.path.exists(ckpt_path):
            test_results = trainer.test(
                model=model,
                datamodule=datamodule,
                ckpt_path=ckpt_path
            )
            log.info(f"Test results with specified checkpoint: {test_results}")
        else:
            log.error(f"Checkpoint path does not exist: {ckpt_path}")
            return {}, object_dict

    # æ–¹å¼3ï¼šä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿›è¡Œæµ‹è¯•
    elif cfg.get("pretrain_path"):
        log.info("Testing with pretrained weights")
        test_results = trainer.test(model=model, datamodule=datamodule)
        log.info(f"Test results with pretrained weights: {test_results}")

    # æ–¹å¼4ï¼šä½¿ç”¨å½“å‰æ¨¡å‹æƒé‡è¿›è¡Œæµ‹è¯•
    else:
        log.info("Testing with current model weights")
        test_results = trainer.test(model=model, datamodule=datamodule)
        log.info(f"Test results with current weights: {test_results}")

    log.info("Testing completed!")

    # è¿”å›æµ‹è¯•ç»“æœå’Œå¯¹è±¡å­—å…¸
    metric_dict = test_results[0] if test_results and len(test_results) > 0 else {}
    return metric_dict, object_dict


@hydra.main(version_base="1.3", config_path="./configs", config_name="test.yaml")
def main(cfg: DictConfig) -> None:
    """æµ‹è¯•ä¸»å…¥å£ç‚¹

    :param cfg: Hydra é…ç½®å¯¹è±¡
    """
    # åº”ç”¨é¢å¤–å·¥å…·
    extras(cfg)

    log.info("Starting model testing process")

    # æ‰§è¡Œæµ‹è¯•
    test_metrics, object_dict = test(cfg)

    # æ‰“å°æµ‹è¯•ç»“æœ
    if test_metrics:
        log.info("=== Final Test Results ===")
        for metric_name, metric_value in test_metrics.items():
            if isinstance(metric_value, (int, float, np.number)):
                log.info(f"{metric_name}: {metric_value:.4f}")
            else:
                log.info(f"{metric_name}: {metric_value}")

        # ç‰¹åˆ«å…³æ³¨å¸¸è§æŒ‡æ ‡
        for key in ['mAP', 'map', 'accuracy', 'Accuracy', 'loss', 'Loss']:
            if key in test_metrics:
                log.info(f"ğŸ¯ Test {key}: {test_metrics[key]:.4f}")
                break
    else:
        log.warning("No test results obtained")


if __name__ == "__main__":
    torch.set_float32_matmul_precision("high")
    main()